{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we try to calculate the parameters of a function ```f(x)``` using the Gradient Descent method. So, let's first create the points distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from math import sin\n",
    "\n",
    "np.random.seed(0)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def genData(numPoints, bias, variance):\n",
    "    x = np.zeros(shape=numPoints)\n",
    "    y = np.zeros(shape=numPoints)\n",
    "    # basically a straight line\n",
    "    x = np.sort(np.random.uniform(-3.14, 3.14, numPoints))\n",
    "    for i in range(0, numPoints):\n",
    "        y[i] = sin(x[i]) + random.uniform(-0.2, 0.2)\n",
    "    return x, y\n",
    "\n",
    "# create points distributions:\n",
    "numPoints = 100\n",
    "x_points, y = genData(numPoints, 25, 10)\n",
    "\n",
    "plt.plot(x_points, y, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit this distribution of points, let's use the following function:\n",
    "```\n",
    "f(x) = a + b·x + c·x² + d·x³\n",
    "```\n",
    "where ```a```, ```b```, ```c``` and ```d``` are constant parameters, also called **weights**.\n",
    "\n",
    "To compute the **Cost**, let's use this function:\n",
    "```\n",
    "Cost = sum((f(x) - y)²)/2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the derivative of the Cost function, we will use the [Chain rule](https://www.khanacademy.org/math/ap-calculus-ab/product-quotient-chain-rules-ab/chain-rule-ab/v/chain-rule-introduction) and for this particular function, we calculate the following gradients:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# TODO: Calculate the Derivative of the Cost function for each of the weights:\n",
    "Gradient_a = \n",
    "Gradient_b = \n",
    "Gradient_c = \n",
    "Gradient_d = \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m denotes the number of examples here, not the number of features\n",
    "def gradientDescent(x, y, weights, learning_rate, num_points, numIterations):\n",
    "    \"\"\"\n",
    "    x: matrix with x values for each equation order\n",
    "    y: array with real values\n",
    "    weights: array with weights\n",
    "    learning_rate: float Learning Rate\n",
    "    num_points: int number of points available in the distribution\n",
    "    numIterations: int with the number of iterations in the Gradient Descent Calculation\n",
    "    \"\"\"\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = np.dot(x, weights)\n",
    "        loss = hypothesis - y\n",
    "        cost = np.sum(loss ** 2) / (2 * num_points)\n",
    "        # print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        \n",
    "        # TODO: Compute the Gradient using loss, x and num_points using np.dot():\n",
    "        gradient = \n",
    "        \n",
    "        # TODO: update weights:\n",
    "        weights -= learning_rate * gradient\n",
    "    \n",
    "    print(\"Final Cost: %f\" % (cost))\n",
    "    return weights\n",
    "\n",
    "\n",
    "# define Gradient Descent parameters: (don't change these values)\n",
    "numIterations = 10000 \n",
    "learning_rate = 0.005\n",
    "\n",
    "# TODO: define equation to fit in Matrix notation in terms of x_points:\n",
    "x = np.asarray([, , , ]).transpose()\n",
    "\n",
    "# initialize weights to ones:\n",
    "weights = np.ones(shape=x.shape[1])\n",
    "\n",
    "# update weights with gradient descent:\n",
    "weights = gradientDescent(x, y, weights, learning_rate, numPoints, numIterations)\n",
    "\n",
    "# weights values:\n",
    "print(\"initial Weights values:\", np.ones(shape=x.shape[1]))\n",
    "print(\"initial Weights final:\", weights)\n",
    "\n",
    "# representation:\n",
    "plt.plot(x_points, y, 'ro')\n",
    "plt.plot(x_points, np.dot(x, weights), 'b-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
